{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Tokens\n",
    "\n",
    "GPT models process text using *tokens*, which are common sequences of characters found in text. The models understand the statistical relationships between these tokens, and excel at producing the next token in a sequence of tokens.\n",
    "\n",
    "The conversion of a prompt into tokens happens automatically when you submit a prompt so you don't need to do anything yourself. However, OpenAI services like Azure OpenAI use the number of tokens processed as part of the pricing model, in the case of Azure OpenAI, charging per 1,000 tokens. So understanding how many tokens your prompts consume is an important part of planning and building any application that will use OpenAI.\n",
    "\n",
    "The prompt **\"Hello world, this is fun!\"** gets tokenized as follows:\n",
    "\n",
    "```\n",
    "Hello\n",
    " world,\n",
    " this\n",
    " is\n",
    " fun\n",
    "!\n",
    "\n",
    "(6 tokens)\n",
    "```\n",
    "Notice how spaces and punctuation are included as part of the tokens. A token doesn't always necessarily equate to a single word or phrase.\n",
    "\n",
    "Let's try the prompt **\"Example using words like indivisible and emojis\"**.\n",
    "\n",
    "```\n",
    "Example\n",
    " using\n",
    " words\n",
    " like\n",
    " ind\n",
    "iv\n",
    "isible\n",
    " and\n",
    " em\n",
    "oj\n",
    "is\n",
    "\n",
    "(11 tokens)\n",
    "```\n",
    "This time you can see that some of the words, **indivisible** and **emojis**, got broken up into smaller chunks.\n",
    "\n",
    "A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words).\n",
    "\n",
    ":thumbsup: You can experiment with this yourself using the *tokenizer* tool available on the OpenAI website at https://platform.openai.com/tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with tokens in code\n",
    "\n",
    "OpenAI provide the `tiktoken` package that you can use to experiment with tokenization in your code.\n",
    "\n",
    "`tiktoken` supports three encodings used by Azure OpenAI Service models:\n",
    "\n",
    "| Encoding name | Azure OpenAI Service models |\n",
    "| ------------- | -------------- |\n",
    "| cl100k_base | gpt-4, gpt-3.5-turbo, text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large |\n",
    "| p50k_base | Code models, text-davinci-002, text-davinci-003 |\n",
    "| r50k_base (or gpt2) | Most GPT-3 models |\n",
    "\n",
    "You can use `tiktoken` as follows to tokenize a string and see what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9906, 1917, 11, 420, 374, 2523, 0]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding.encode(\"Hello world, this is fun!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was the output of the above code what you were expecting?\n",
    "\n",
    "If you were expecting text broken up like the examples at the top of this page, you were probably wondering why you just got back a bunch of seemingly random numbers. This is because the AI models don't work on words. Instead, they use a method called *BPE* (Byte Pair Encoding) to convert the text into numeric tokens.\n",
    "\n",
    "One of the features of BPE is that it's reversible, so you can convert the tokens back into the original text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the text instead of the tokens\n",
    "\n",
    "Check out the following code that displays the human readable text instead of the token values.\n",
    "\n",
    ":bulb: **NOTE:** See the following cookbook for some tips on working with `tiktoken`: [How to count tokens with tiktoken](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'Hello', b' world', b',', b' this', b' is', b' fun', b'!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write code to display the text from the tokens below\n",
    "[encoding.decode_single_token_bytes(token) for token in encoding.encode(\"Hello world, this is fun!\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see results similar to the following\n",
    "\n",
    "`[b'Hello', b' world', b',', b' this', b' is', b' fun', b'!']`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a function to return the number of tokens\n",
    "\n",
    "Using what you've learned so far, let's create a function that returns the count of the number of tokens in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_num_tokens_from_string(string: str, encoding_name: str='cl100k_base') -> int:\n",
    "    \"\"\"Returns the number of tokens in a text by a given encoding.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "get_num_tokens_from_string(\"Hello World, this is fun!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blowing up the prompt!\n",
    "\n",
    "Apart from cost, there's another reason that you'll want to be in control of the number of tokens you use. All AI models have a limit on the maximum number of tokens that a request can consume. The limit per request includes the number of tokens in the prompt **plus** the number of tokens in the response. Different models can have different token limits, but ultimately the overall size of your prompt and the response to that prompt have to be smaller than that limit.\n",
    "\n",
    "Let's show this in action by deliberately sending a prompt that's too large."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a file, `movies.csv`, in this folder which contains a long list of movie data. Let's use `tiktoken` to see how big this file is in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 59233\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "\n",
    "# Open the file with information about movies\n",
    "movie_data = os.path.join(os.getcwd(), \"movies.csv\")\n",
    "content = open(movie_data, \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "# Use tiktoken to tokenize the content and get a count of tokens used.\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print (f\"Token count: {len(encoding.encode(content))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have a result of something like 60,000 tokens. Which is a huge amount of tokens! But let's continue regardless.\n",
    "\n",
    "Let's setup the OpenAI API to use for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen in the Prompts section of this workshop, we can provide additional data to an AI model by including that data in the prompt. So, let's construct a prompt that asks for the highest rated movie and then provides the list of movies for the AI to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the highest rated movie from the following list\n",
      "CSV list of movies:\n",
      "id,original_language,original_title,popularity,release_date,vote_average,vote_count,genre,overview,revenue,runtime,tagline\n",
      "381284.0,en,Hidden Figures,49.802,2016-12-10,8.1,7310.0,\"['Drama', 'History']\",\"The untold story of Katherine G. Johnson, Dorothy Vaughan and Mary Jackson – brilliant African-American women working at NASA and serving as the brains behind one of the greatest operations in history – the launch of astronaut John Glenn into orbit. The visionary trio crossed all gender and race lines to inspire generations to dream big.\",230698791.0,127.0,\"Meet the women you don't know, behind the mission you do.\"\n",
      "356334.0,en,Gridlocked,9.801,2016-06-14,5.8,130.0,['Action'],Former SWAT leader David Hendrix and hard-partying movie star Brody Walker must cut their ride-along short when a police training facility is attacked by a team of mercenaries.,0.0,114.0,Only one way out…\n",
      "475557.0,en,Joker,116.462,2019-10-02,8.2,18970.0,\"['Crime', 'Thriller', 'Drama']\",\"During the 1980s, a failed stand-up comedian is driven insane and turns to a life of crime and chaos in Gotham City while becoming an infamous psychopathic crime figure.\",1074251311.0,122.0,Put on a happy face.\n",
      "347847.0,en,The Sand,14.172,2015-08-28,5.1,157.0,['Horror'],\"Just when you thought it was safe to go back in the water again, you can’t even get across the sand! BLOOD BEACH meets SPRING BREAKERS in an ace monster mash-up of smart nostalgia and up-to-the-minute visual effects. After an all-night graduation beach party, a group of hung-over students wake up under blazing sun to find their numbers somewhat depleted. An enormous alien creature has burrowed down deep and anyone foolish enough to make contact with the sand finds themselves at the mercy of a sea of flesh-eating tentacles. Will they ever be able to escape its carnivorous clutches?\",0.0,84.0,This Beach is Killer\n",
      "739542.0,en,America: The Motion Picture,98.542,2021-06-30,5.8,130.0,\"['Action', 'Comedy', 'History', 'Animation', 'Fantasy']\",A chainsaw-wielding George Washington teams with beer-loving bro Sam Adams to take down the Brits in a tongue-in-cheek riff on the American Revolution.,0.0,98.0,This summer they're redrawing history.\n",
      "505262.0,ja,僕のヒーローアカデミア THE MOVIE ～2人の英雄～,220.805,2018-08-03,8.0,677.0,\"['Animation', 'Action', 'Adventure', 'Fantasy']\",\"All Might and Deku accept an invitation to go abroad to a floating and mobile manmade city, called 'I-Island', where they research quirks as well as hero supplemental items at the special 'I-Expo' convention that is currently being held on the island. During that time, suddenly, despite an iron wall of security surrounding the island, the system is breached by a villain, and the only ones able to stop him are the students of Class 1-A.\",31478826.0,96.0,Who is your hero?\n",
      "3512.0,en,Under Siege 2: Dark Territory,19.217,1995-07-13,5.7,608.0,\"['Action', 'Thriller']\",\"A passenger train has been hijacked by an elec ...[211210 characters]\n"
     ]
    }
   ],
   "source": [
    "query = \"What's the highest rated movie from the following list\\n\"\n",
    "query += \"CSV list of movies:\\n\"\n",
    "query += content\n",
    "\n",
    "print (f\"{query[0:3000]} ...[{len(query)} characters]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we've output the first few lines of the prompt and we've printed the overall size (in characters) of the prompt. Now, let's see what happens if we submit that query to the AI.\n",
    "\n",
    "**NOTE:** This exercise is intended to fail, but it does depend on the model you're using. Some newer models have a higher token limit than others. If you're using a model with a high token limit, you might want to increase the size of the `movies.csv` file to make this fail, or you could try using a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\"),\n",
    "    messages = [{\"role\" : \"assistant\", \"content\" : query}],\n",
    ")\n",
    "\n",
    "print (response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This request will fail. At the end of the output, you should see an error message something like the following:\n",
    "\n",
    "```bash\n",
    "InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 59265 tokens. Please reduce the length of the messages.\n",
    "```\n",
    "\n",
    "You can see quite clearly from the error message that we've exceeded the model's maximum length. Its maximum is **16385** tokens, we've sent a request requiring **59265** tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, we've learned about tokens. Your prompts will be broken up and sent to the AI as tokens and all AI models have a maximum token size which you must take care not to exceed. You will also be charged based on the number of tokens that your queries consume."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "In the next lab, we'll look at one of the ways you can take control of the number of tokens your prompts consume by introducing the concept of **Embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Section\n",
    "\n",
    "📣 [Embeddings](../02-Embeddings/embeddings.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibit-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
