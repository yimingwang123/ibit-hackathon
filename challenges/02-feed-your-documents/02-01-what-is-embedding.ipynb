{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-01 - Was sind Embeddings\n",
    "\n",
    "In diesem Lab werden wir untersuchen, wie wir unsere eigenen Daten in die von Azure OpenAI verwendeten Modelle einbringen können.\n",
    "\n",
    "Wir beginnen wie üblich damit, eine Verbindung zum Azure OpenAI-Dienst herzustellen.\n",
    "\n",
    "**HINWEIS**: Wie in den vorherigen Labs verwenden wir die Werte aus der Datei `.env`, die sich im Hauptverzeichnis dieses Repositorys befindet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Laden der Umgebungsvariablen\n",
    "if load_dotenv():\n",
    "    print(\"Azure OpenAI-Endpunkt gefunden: \" + os.getenv(\"AZURE_OPENAI_ENDPOINT\"))\n",
    "else: \n",
    "    print(\"Keine .env-Datei gefunden\")\n",
    "\n",
    "# Erstellen einer Instanz von Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lassen Sie uns damit beginnen, der KI eine einfache Frage zu stellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = llm.invoke(\"Tell me about the latest Deadpool movie. When was it released? What is it about?\")\n",
    "\n",
    "# Ausgabe der Antwort\n",
    "print(r.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was fällt Ihnen an der Antwort auf?\n",
    "\n",
    "Der neueste \"Deadpool\"-Film heisst \"Deadpool and Wolverine\". Abhängig vom Modell und der Version, die Sie verwenden, kann es sein, dass das Modell einen der vorherigen Filme als den neuesten bezeichnet oder dass es zwar weiß, dass es einen neuen Film gibt, aber denkt, dass dieser noch nicht veröffentlicht wurde.\n",
    "\n",
    "OpenAI-Modelle werden auf einer großen Menge Daten trainiert, aber dies geschieht zu einem bestimmten Zeitpunkt, der vom jeweiligen Modell abhängt. Daher haben viele Modelle keine Informationen über Ereignisse, die in sehr aktuellen Monaten oder Jahren stattgefunden haben.\n",
    "\n",
    "Um der KI zu helfen, können wir ihr zusätzliche Informationen bereitstellen. Das gleiche Vorgehen würden Sie anwenden, wenn Sie möchten, dass die KI mit Ihren firmeneigenen Daten arbeitet. Die KI weiß nichts über Informationen, die nicht öffentlich zugänglich sind. Wenn Sie also möchten, dass die KI mit diesen Informationen arbeitet, müssen Sie diese Informationen in das Modell einbringen.\n",
    "\n",
    "Das Problem ist, dass Sie das Modell dazu nicht wirklich neu trainieren können. Die Modelle sind vorab trainiert, und ein erneutes Training ist aufwendig und zeitintensiv.\n",
    "\n",
    "Es gibt jedoch Möglichkeiten, KI-Modelle trotzdem mit neuen Daten arbeiten zu lassen. Die beliebteste Methode ist die Verwendung von *Embeddings* (Einbettungen), die wir in den nächsten Abschnitten untersuchen werden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigene Daten einbringen\n",
    "\n",
    "Langchain bietet eine Reihe nützlicher Tools, darunter solche, die den Umgang mit externen Dokumenten vereinfachen. Im folgenden Beispiel verwenden wir den `DirectoryLoader`, der mehrere Dateien aus einem Verzeichnis lesen kann, und den `UnstructuredMarkdownLoader`, der Dateien im Markdown-Format verarbeiten kann. Wir nutzen diese, um eine Reihe von Markdown-Dateien zu verarbeiten, die Details über kürzlich veröffentlichte Filme enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "\n",
    "data_dir = \"movies\"\n",
    "\n",
    "documents = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader).load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben nun ein Objekt `documents`, das alle Informationen aus unseren Markdown-Dokumenten über Filme enthält.\n",
    "\n",
    "Wir können die `question_answering`-Chain verwenden, um der KI Zugriff auf unsere Dokumente zu geben und dann dieselbe Frage zu den Deadpool-Filmen erneut stellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frage-Antwort-Chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Vorbereiten der Chain und der Abfrage\n",
    "chain = load_qa_chain(llm)\n",
    "query = \"Tell me about the latest Deadpool movie. When was it released? What is it about?\"\n",
    "\n",
    "result = chain.invoke({'input_documents': documents, 'question': query})\n",
    "\n",
    "print (result['output_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super! Das Modell kennt nun die richtigen Details zum neuesten Deadpool-Film.\n",
    "\n",
    "Allerdings gibt es etwas im Hintergrund zu beachten! Werfen wir einen Blick darauf, was hinter den Kulissen passiert.\n",
    "\n",
    "Wir werden zwei Dinge tun. Erstens werden wir den Parameter `verbose=True` zur Chain hinzufügen, und zweitens werden wir die Ausführung der Chain in einen Callback einbetten, um die Anzahl der verbrauchten Tokens zu erfassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unterstützung für Callbacks\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Vorbereiten der Chain und der Abfrage\n",
    "chain = load_qa_chain(llm, verbose=True)\n",
    "query = \"Tell me about the latest Deadpool movie. When was it released? What is it about?\"\n",
    "\n",
    "# Ausführen der Chain unter Verwendung des Callbacks, um die Anzahl der verwendeten Tokens zu erfassen\n",
    "with get_openai_callback() as callback:\n",
    "    chain.invoke({'input_documents': documents, 'question': query})\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Gesamtanzahl verwendeter Tokens: {total_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Ausgabe des letzten Codeabschnitts sollten Sie viele Informationen sehen. Am Ende sollte die Anzahl der verwendeten Tokens angezeigt werden. Sie werden möglicherweise überrascht sein, dass die Abfrage je nach verwendetem Modell zwischen 5000 und 6000 Tokens in Anspruch nehmen kann. Das ist eine Menge!\n",
    "\n",
    "Mit der Option `verbose=True` wird in der restlichen Ausgabe auch das Prompt angezeigt, das für die Abfrage zusammengestellt wurde. Wenn Sie durch die Ausgabe scrollen, werden Sie feststellen, dass das Prompt **alle** Informationen aus unseren Dokumenten enthält – daher werden so viele Tokens verbraucht.\n",
    "\n",
    "Wie wir bereits besprochen haben, haben KI-Modelle eine maximale Anzahl von Tokens, die verwendet werden können, und ein Preismodell, das auf der Anzahl der verbrauchten Tokens basiert. In diesem Beispiel sind die Dokumente relativ klein und es gibt nur 20 davon, aber wenn wir mit größeren Dokumenten und mehr davon arbeiten wollten, würde diese Methode schnell kostspielig werden und schließlich die Token-Grenze erreichen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Die Lösung für den Umgang mit großen Datenmengen aus externen Quellen besteht in der Verwendung von *Embeddings*. OpenAI stellt Embedding-Modelle zur Verfügung, mit denen menschenlesbare Informationen nach Bedeutung und Intention analysiert werden können. Die Ausgabe eines Embedding-Modells sind numerische Daten, sogenannte *Vektoren*. Diese ermöglichen es Computern, ähnliche Informationen zu gruppieren. Die Vektoren werden dann in einem *Vektorstore* gespeichert. Wenn Sie eine Frage stellen möchten, wird der Abfragetext erneut mithilfe eines Embedding-Modells in Vektoren umgewandelt. Diese Vektor-Daten, die Ihre Abfrage repräsentieren, können dann in der Datenbank durchsucht werden. Die gefundenen ähnlichen Vektoren sind vermutlich eine gute Antwort auf Ihre Frage.\n",
    "\n",
    "Um zu verhindern, dass ein Prompt mit einer großen Anzahl an Tokens überlastet wird, senden wir nicht mehr alle unsere Dokumente an die KI, sondern führen zuerst eine Vektorsuche durch, um eine relevante Teilmenge der Dokumente zu finden. Nur diese relevante Teilmenge wird dann in das Prompt eingebunden."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginnen wir mit der Initialisierung einer Instanz eines Embedding-Modells. Sie werden feststellen, dass dies ähnlich ist, wie wenn wir eine unserer Modelldepoyments für Anfragen initialisieren, nur dass wir in diesem Fall ein Embedding-Modell angeben. Typischerweise wurde hierfür `text-embedding-ada-002` verwendet, aber inzwischen gibt es neuere Alternativen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings_model = AzureOpenAIEmbeddings(    \n",
    "    azure_deployment = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    model= os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt, wo wir ein Modell zur Erstellung von Embeddings initialisiert haben, können wir beginnen, unsere Dokumente einzubetten.\n",
    "\n",
    "Wie im vorherigen Beispiel verwenden wir den in Langchain integrierten Loader, um die Dokumente aus einem Verzeichnis zu laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader).load()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nächste Schritt ist die Verwendung eines *Splitters*. Ein Splitter ermöglicht es uns, längere Dokumente in kleinere Teile zu zerlegen, damit wir nicht riskieren, das Token-Limit zu überschreiten, wenn wir unsere Daten an das Embedding-Modell senden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "document_chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt werden die aufgeteilten Dokumente in Vektoren konvertiert, indem wir die Daten durch ein Embedding-Modell schicken. Die resultierenden Vektoren werden in einer Vektor-Datenbank gespeichert. In diesem Beispiel verwenden wir die Datenbank **Qdrant** (ausgesprochen „Quadrant“). Wir initialisieren sie mit der Option `location=\":memory:\"`, damit die Datenbank im Speicher und nicht auf der Festplatte gespeichert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    document_chunks,\n",
    "    embeddings_model,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"movies\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der obige Codeabschnitt kümmert sich um die Initialisierung der Qdrant-Datenbank, das Durchführen der Dokumente durch das Embedding-Modell und das Speichern der resultierenden Vektoren in der Datenbank.\n",
    "\n",
    "Als Nächstes definieren wir einen *Retriever*. In Langchain ist ein Retriever ein Interface, das Suchergebnisse aus dem Vektorspeicher zurückgibt. Wir erstellen also einen Retriever für unsere Qdrant-Datenbank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Nächstes definieren wir eine `RetrievalQA`-Chain. Diese kümmert sich um den Prozess, eine Frage zu beantworten, indem zuerst in der Vektor-Datenbank gesucht und anschließend die Ergebnisse an unser KI-Modell übergeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt führen wir unsere Abfrage erneut aus – mit einer kleinen Änderung.\n",
    "\n",
    "Möglicherweise denken Sie: „Es ist nicht überraschend, dass die KI jetzt über den neuesten Deadpool-Film Bescheid weiß, schließlich haben wir sie darüber informiert!“ Also versuchen wir zu zeigen, dass die KI tatsächlich ein wenig nachdenken und Schlüsse ziehen kann.\n",
    "\n",
    "Falls Sie kein Fan dieser Filme sind: Deadpool entstammt den Marvel-Comics. Alle Filme, die aus Marvel-Comics hervorgehen, werden üblicherweise als Teil des Marvel Cinematic Universe betrachtet (oft MCU genannt). Wir haben in unseren Daten weder Marvel noch MCU erwähnt. Wenn wir die Frage also ein wenig anpassen und nach dem MCU statt nach Deadpool fragen, sollte die KI durch Nachdenken erkennen, was gemeint ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the latest MCU movie. When was it released? What is it about?\"\n",
    "result = qa.invoke(query)\n",
    "print(result['result'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn alles geklappt hat, sollte die KI geantwortet haben, dass der neueste MCU-Film \"Deadpool & Wolverine\" ist, der im Juli 2024 veröffentlicht wurde.\n",
    "\n",
    "Wir bekommen also die erwartete Antwort, doch schauen wir uns nun den Hauptgrund an, weshalb wir diesen ganzen Aufwand betreiben. Wurde die Anzahl der verwendeten Tokens verringert? Nutzen wir wieder unsere Callback-Technik, um das herauszufinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as callback:\n",
    "    qa.invoke(query)\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Gesamtanzahl verwendeter Tokens: {total_tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die genaue Anzahl der Tokens kann variieren, aber es sollte offensichtlich sein, dass bei dieser Abfrage nun deutlich weniger Tokens verbraucht werden, oft um rund 2.000 weniger.\n",
    "\n",
    "KI-Orchestrierungsbibliotheken wie Langchain und Semantic Kernel können den Prozess des Einbettens, Vektorisierens und Suchens stark vereinfachen. Im vorherigen Abschnitt haben wir den Prozess des Dokument-Splittings, Einbettens, Vektorisierens, Speicherns von Vektoren in einer Datenbank und Erstellen eines Retrievers Schritt für Schritt durchlaufen. Im nächsten Abschnitt verwenden wir erneut den `DirectoryLoader` von Langchain, um unsere Markdown-Dokumente zu laden und zu verarbeiten. Diesmal nutzen wir jedoch einen `VectorstoreIndexCreator`. Sie werden sehen, dass nur wenige Parameter nötig sind – das zu verwendende Embedding-Modell und die Quelldaten (`loader`). Hinter den Kulissen führt der `VectorstoreIndexCreator` allerdings sämtliche Schritte aus, die wir manuell in den vorherigen Abschnitten ausgeführt haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "loader = DirectoryLoader(path=data_dir, glob=\"*.md\", show_progress=True, loader_cls=UnstructuredMarkdownLoader)\n",
    "\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=embeddings_model\n",
    "    ).from_loaders([loader])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um nun eine Abfrage gegen unsere Daten zu starten, müssen wir nur das Prompt angeben und dann den oben erstellten Index aufrufen, dem wir das Modell (`llm`) und die Frage übergeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the latest Deadpool movie. When was it released? What is it about?\"\n",
    "index.query(llm=llm, question=query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie sehen, dies ist eine sehr einfache Möglichkeit, Embeddings und Vektoren in einer KI-Anwendung einzusetzen und eignet sich hervorragend, um schnell zu starten.\n",
    "\n",
    "Wir können erneut die Callback-Methode nutzen, um sicherzustellen, dass weiterhin eine geringere Anzahl an Tokens verbraucht wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausführen der Chain, um die Anzahl der verwendeten Tokens zu erfassen\n",
    "with get_openai_callback() as callback:\n",
    "    index.query(llm=llm, question=query)\n",
    "    total_tokens = callback.total_tokens\n",
    "\n",
    "print(f\"Gesamtanzahl verwendeter Tokens: {total_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibit-hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
