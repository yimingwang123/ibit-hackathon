{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02-03 - Vector Store mit Azure AI Search\n",
        "\n",
        "In diesem Notebook werden wir uns ansehen, wie wir Azure AI Search als Vektor-Store verwenden können – einschließlich der verschiedenen Suchmethoden, die der Dienst unterstützt – und wie man dies für Retrieval Augmented Generation (RAG) mit großen Sprachmodellen einsetzen kann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Übersicht\n",
        "\n",
        "1. **Erklärung: Erstellen eines AI Search-Indexes über das Azure Portal (Cosmos DB UI)**\n",
        "2. **Verbindung zu Azure AI Search herstellen**\n",
        "3. **Vektor-Suche verwenden**\n",
        "4. **Hybride Suche**\n",
        "5. **RAG mit Langchain und Azure AI Search**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Erklärung: Erstellen eines **AI Search**-Indexes über das **Azure Portal** (**Cosmos DB** UI)\n",
        "\n",
        "Wenn bereits Daten in **Cosmos DB** vorhanden sind, kann man im **Azure Portal** direkt einen **AI Search**-Index (auch „Azure Cognitive Search Index“) anhand dieser Cosmos DB-Daten erstellen.\n",
        "\n",
        "---\n",
        "\n",
        "### Schritte (im Azure Portal):\n",
        "\n",
        "1. Erstelle zunächst einen neuen **AI Search**-Service in deiner **resource group**, indem du im Azure Marketplace danach suchst\n",
        "\n",
        "![Azure AI Search Service](images/find-ai-search.png)\n",
        "\n",
        "Konfiguriere rasch die Ressourcengruppe, Netzwerkverbindung (public internet) und lege den Dienst an.\n",
        "\n",
        "![Konfiguration](images/config-ai-search.png) \n",
        "\n",
        "2. Öffne nun die Ressource **Cosmos DB** im **Azure Portal**, in der deine Daten liegen.  \n",
        "3. Im linken Menü gibt es eine Option **Integrations** und dann **Add Azure AI Search**. Klicke darauf.  \n",
        "4. Wähle den zuvor erstellten **AI Search**-Service und klicke auf „connect to your data“. Halte dich an den Assistenten und überspringe „add cognitive skills“.  \n",
        "5. Unter „Customize target index“ achte darauf, dass alle Spalten außer der „vector column“ den Typ „Emd.String“ haben. Für die „vector column“ ist der Datentyp **Collection(Edm.Single)** erforderlich.\n",
        "\n",
        "**WICHTIG:** Alle Spalten müssen **Searchable** und **Retrievable** sein (für die Semantic Suche später)!\n",
        "\n",
        "6. Klicke auf **Configure vector field** und gib z. B. 3072 als Dimension an\n",
        "\n",
        "![AI Search Index](images/config-aisearch-index.png)\n",
        "\n",
        "Konfiguriere Algorithmus und „vectorizer“ mit den Standardeinstellungen. Für diesen Hackathon ist keine Komprimierung nötig.  \n",
        "\n",
        "\n",
        "\n",
        "7. Abschließend klicke auf **create indexer**. Dort kannst du einen Zeitplan festlegen, um deine Datenbasis automatisch aus Cosmos DB zu aktualisieren. Für den Hackathon reicht „once“.  \n",
        "8. Sobald der Indexer erstellt ist, werden deine Cosmos DB-Dokumente indexiert. Dies kann ein paar Minuten dauern.\n",
        "\n",
        "---\n",
        "\n",
        "### Nach dem Erstellen des Indexers:\n",
        "\n",
        "1. Wähle deinen **Azure Cognitive Search Service** aus.  \n",
        "2. Dort siehst du:  \n",
        "   - **Indexer**: Zeigt dir, ob der Indexer erfolgreich war oder fehlgeschlagen ist. Du kannst auch genauer sehen, wann er das letzte Mal lief.  \n",
        "   - **Indexes**: Zeigt dir die tatsächlich erstellten Suchindizes. Ist dein Indexer erfolgreich durchgelaufen, findest du hier den neuen Index.  \n",
        "   - **Spalten / Felder wählen**: Entscheide, welche Felder in der Suche auftauchen sollen (z. B. Titel, Beschreibung, Genre). Bestimme, welches Feld als Schlüssel (`key`) dient.  \n",
        "   - **Optional**: Vektor-Feld definieren — falls du **Vektorfelder** (Embeddings) in deinen Cosmos-Daten hast, kannst du dieses Feld sowie dessen Dimension (z. B. 1536 für `text-embedding-ada-002`) angeben.  \n",
        "   - Abschließend **Speichern** / **Erstellen**.\n",
        "\n",
        "Sobald dies abgeschlossen ist, kannst du direkt auf diesen Index zugreifen und in diesem Notebook gegen **AI Search**-Abfragen starten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Verbindung zu Azure AI Search herstellen\n",
        "\n",
        "Bevor wir anfangen können, müssen wir unser Python-Umfeld entsprechend konfigurieren, damit wir auf unsere Azure-Dienste zugreifen können. Wie in den anderen Beispielen verwenden wir eine `.env`-Datei mit folgenden Schlüsseln. Die Werte liegen bei AI Search Service unter dem Tab **Settings -> Keys** oder im **Overview** Tab.\n",
        "\n",
        "```\n",
        "AZURE_AI_SEARCH_SERVICE_NAME = \"<DEIN AI SEARCH NAME>\"\n",
        "AZURE_AI_SEARCH_ENDPOINT = \"<DEIN AI SEARCH ENDPOINT URL>\"\n",
        "AZURE_AI_SEARCH_INDEX_NAME = \"<DEIN AI SEARCH INDEX NAME>\"\n",
        "AZURE_AI_SEARCH_API_KEY = \"<DEINE AI SEARCH ADMIN API KEY>\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schritt 1: Laden der Umgebungsvariablen\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "if load_dotenv():\n",
        "    print(\"Env-Datei wurde erfolgreich geladen.\")\n",
        "else:\n",
        "    print(\"Keine Env-Datei gefunden.\")\n",
        "\n",
        "azure_ai_search_service_name = os.getenv(\"AZURE_AI_SEARCH_SERVICE_NAME\")\n",
        "azure_ai_search_endpoint = os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\")\n",
        "azure_ai_search_index_name = os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\")\n",
        "azure_ai_search_api_key = os.getenv(\"AZURE_AI_SEARCH_API_KEY\")\n",
        "\n",
        "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "azure_openai_completion_deployment_name = os.getenv(\"AZURE_OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
        "azure_openai_embedding_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
        "\n",
        "print(\"Verwende Azure AI Search:\", azure_ai_search_service_name)\n",
        "print(\"Verwende Index:\", azure_ai_search_index_name)\n",
        "print(\"Verwende Azure OpenAI Endpoint:\", azure_openai_endpoint)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Schüsselwortsuche (Keyword Search)\n",
        "Für alle Suchtypen müssen wir immer zuerst ein **Search Client** initialisieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azure.search.documents import SearchClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents.models import VectorizedQuery\n",
        "\n",
        "# SearchClient initialisieren\n",
        "search_client = SearchClient(\n",
        "    endpoint=azure_ai_search_endpoint,\n",
        "    index_name=azure_ai_search_index_name,\n",
        "    credential=AzureKeyCredential(azure_ai_search_api_key)\n",
        ")\n",
        "\n",
        "print(\"Azure Search Client erfolgreich initialisiert.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Zuerst führen wir eine einfache Schlüsselwortsuche durch. \n",
        "\n",
        "Wir erhalten nur ein Ergebnis, aber das ist nicht vollständig. Es könnte sein, dass im Index irgendwo das Wort \"hero\" enthalten ist, beispielsweise in der Beschreibung – etwa bei \"heroische Taten\" oder ähnlichem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Einfache Schlüsselwortsuche durchführen\n",
        "query = \"hero\"\n",
        "\n",
        "results = list(search_client.search(\n",
        "    search_text=query,\n",
        "    query_type=\"simple\",\n",
        "    include_total_count=True,\n",
        "    top=5\n",
        "))\n",
        "\n",
        "for result in results:\n",
        "    print(\"Film: {}\".format(result[\"original_title\"]))\n",
        "    print(\"Genre: {}\".format(result[\"genre\"]))\n",
        "    print(\"----------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Nächster Schritt: Eine Frage stellen statt nur ein Schlüsselwort zu suchen\n",
        "Nun probieren wir dasselbe erneut, aber diesmal formulieren wir eine Frage, anstatt nur nach einem einzelnen Wort zu suchen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Was sind die besten Filme über Superhelden?\"\n",
        "\n",
        "results = list(search_client.search(\n",
        "    search_text=query,\n",
        "    query_type=\"simple\",\n",
        "    include_total_count=True,\n",
        "    top=5\n",
        "))\n",
        "\n",
        "for result in results:\n",
        "    print(\"Film: {}\".format(result[\"original_title\"]))\n",
        "    print(\"Genre: {}\".format(result[\"genre\"]))\n",
        "    print(\"----------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wie zuvor sind die Ergebnisse gemischt. Einige Filme könnten tatsächlich etwas mit Superhelden zu tun haben, andere aber nicht. Der Grund dafür ist, dass die Suche weiterhin auf Schlüsselwörtern basiert.\n",
        "\n",
        "#### Nächster Schritt: Eine Vektor-Suche ausprobieren\n",
        "Um genauere Ergebnisse zu erhalten, testen wir als Nächstes eine Vektorsuche, um die Bedeutung der Suchanfrage besser zu erfassen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Vektor-Suche verwenden\n",
        "\n",
        "Angenommen, die Daten liegen bereits im Cosmos DB-Container (siehe letzte Übung) und wurden über die oben beschriebenen Schritte in unseren Azure AI Search Index eingepflegt. Wir benötigen also **keinen** erneuten CSV-Import oder ähnliches.\n",
        "\n",
        "Jetzt können wir eine Vektor-Suche gegen diesen Index durchführen. Dazu verwenden wir unsere Embedding-Funktion über Azure OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Azure OpenAI Embeddings\n",
        "\n",
        "Wir verwenden Azure OpenAI Embeddings, um unseren Such-String als Vektor zu kodieren. Anschliessend übergeben wir diesen Vektor an den Azure Search Dienst (Vektor-Suche)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "\n",
        "azure_openai_embeddings = AzureOpenAIEmbeddings(\n",
        "    azure_deployment=azure_openai_embedding_deployment_name\n",
        ")\n",
        "print(\"Embedding-Client mit Azure OpenAI initialisiert.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Einfache Vektor-Suche\n",
        "\n",
        "Wir starten mit einer reinen Vektor-Suche, d.h. wir übergeben keinen \"Search Text\" an den Index, sondern nur den aus Embeddings generierten Vektor. Wir sehen uns an, welche Ergebnisse zurückkommen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_query = \"What are the best movies about superheroes?\"\n",
        "\n",
        "# Embedding erstellen\n",
        "query_vector = azure_openai_embeddings.embed_query(user_query)\n",
        "vector_query = VectorizedQuery(\n",
        "    vector=query_vector,\n",
        "    k_nearest_neighbors=5,  # Anzahl der ähnlichen Treffer\n",
        "    fields=\"vector\"  # Das Feld im Index, das Embeddings enthält\n",
        ")\n",
        "\n",
        "# Alle durchsuchbaren Felder (entsprechend den Feldern in deinem Index)\n",
        "searchable_fields = [\n",
        "    \"original_language\", \"original_title\", \"popularity\", \"release_date\", \n",
        "    \"vote_average\", \"vote_count\", \"genre\", \"overview\", \"revenue\", \n",
        "    \"runtime\", \"tagline\"\n",
        "]\n",
        "\n",
        "# Suchanfrage an Azure AI Search\n",
        "search_results = search_client.search(\n",
        "    search_text=None,  # Reine Vektorsuche, kein Text\n",
        "    search_fields=searchable_fields,  # Suche in allen relevanten Feldern\n",
        "    vector_queries=[vector_query],  # Vektorsuche aktiv\n",
        "    top=5\n",
        ")\n",
        "\n",
        "results_list = list(search_results)\n",
        "for result in results_list:\n",
        "    print(f\"Titel: {result['original_title']} | Genre: {result.get('genre', 'keine Angabe')} | Score: {result['@search.score']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Die Ergebnisse könnten nicht genau das sein, was wir erwarten. Die Vektor-Suche liefert hier z.B. Ähnlichkeit auf Basis von Vektoren (Embeddings). Manchmal sind die Ergebnisse gut, manchmal aber nicht 100% zutreffend, weil wir rein die Vektor-Ähnlichkeit verwenden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Hybride Suche\n",
        "\n",
        "Azure AI Search bietet die Möglichkeit, eine **Hybrid-Suche** durchzuführen, bei der sowohl Keywords (volltextbasiert) als auch Vektoren (Embeddings) berücksichtigt werden. Weiterhin kann der semantische Ranker von Azure Cognitive Search die Ergebnisse neu bewerten.\n",
        "\n",
        "Dies kann bessere Ergebnisse liefern, da man einerseits das Kontextverständnis der Vektoren nutzt und andererseits die \"exakten\" Keyword-Treffer beibehält."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Füge ein Semantic Search Config hinzu:\n",
        "\n",
        "In diesem Abschnitt richten wir **semantische Suche** ein, um die Suchergebnisse nicht nur basierend auf Schlüsselwörtern, sondern auch auf die tatsächliche Bedeutung der Anfrage zu verbessern.\n",
        "\n",
        "Ziel:\n",
        " - Wir definieren **wichtige Felder** für die semantische Suche.\n",
        " - Dadurch können wir relevantere Treffer erzielen.\n",
        "\n",
        "Die semantische Suche analysiert den Kontext von Suchanfragen und den Inhalt der indexierten Daten, um intelligentere Suchergebnisse zu liefern.\n",
        "\n",
        "Gehe zum deinem Index im AI Search auf Azure Portal, und mache folgende Konfiguration:\n",
        "\n",
        "![Semantic Search Configuration](images/semantic-config.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Durchführung der hybriden Suche\n",
        "\n",
        "In diesem Schritt kombinieren wir sowohl den Klartext der Suchanfrage als auch den Embedding-Vektor, der die semantische Bedeutung der Anfrage repräsentiert. Azure AI Search wird diese Informationen nutzen, um die Suchergebnisse zu verbessern, indem es Keyword-Übereinstimmungen, Vektor-Ähnlichkeiten und (optional) semantische Ranking-Algorithmen kombiniert.\n",
        "\n",
        "Durch diese hybride Suche können wir sicherstellen, dass die Ergebnisse sowohl auf exakten Schlüsselwörtern als auch auf der semantischen Bedeutung der Anfrage basieren. Dies führt zu präziseren und relevanteren Suchergebnissen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wir übergeben sowohl den Text als auch den Embedding-Vektor.\n",
        "# Azure AI Search wird Keyword + Vektor + (optional) semantische Ranking-Algorithmen kombinieren.\n",
        "\n",
        "user_query = \"What are the best movies about superheroes?\"\n",
        "query_vector = azure_openai_embeddings.embed_query(user_query)\n",
        "vector_query = VectorizedQuery(\n",
        "    vector=query_vector,\n",
        "    k_nearest_neighbors=5, \n",
        "    fields=\"vector\"\n",
        ")\n",
        "\n",
        "hybrid_search_results = search_client.search(\n",
        "    search_text=user_query,  # jetzt übergeben wir den Klartext\n",
        "    vector_queries=[vector_query],\n",
        "    query_type=\"semantic\",  # semantische Suche aktivieren\n",
        "    semantic_configuration_name=\"movies-semantic-config\",\n",
        "    select=[\"original_title\", \"genre\"],\n",
        "    top=5\n",
        ")\n",
        "# Füge semantische Suche hinzu\n",
        "\n",
        "hybrid_list = list(hybrid_search_results)\n",
        "\n",
        "for r in hybrid_list:\n",
        "    print(\"Movie:\", r[\"original_title\"], \"| Genre:\", r.get(\"genre\", \"N/A\"))\n",
        "    print(\"   Score:\", r[\"@search.score\"], \" Reranker Score:\", r.get(\"@search.reranker_score\", \"n/a\"))\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Meistens erhalten wir mit dieser Methode (Hybrid + Semantische Reranking) bessere Ergebnisse, da die Keyword-Übereinstimmungen UND Vektor-Ähnlichkeiten UND der semantische Ranker kombiniert werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Retrieval Augmented Generation (RAG) + Langchain\n",
        "\n",
        "In diesem Abschnitt wollen wir die gefundenen Dokumente (Film-Infos) als Kontext an ein grosses Sprachmodell (z.B. GPT) geben, um eine konkrete Antwort zu generieren. Dies ist das übliche RAG-Muster:\n",
        "\n",
        "1. Frage an das System\n",
        "2. Erstelle Embeddings der Frage\n",
        "3. Suche in Azure AI Search (oder einer anderen Vektor-Datenbank)\n",
        "4. Nimm die Top-Treffer als \"Kontext\" (z.B. Film-Details) und baue daraus ein Prompt\n",
        "5. Frage das LLM (z.B. Azure OpenAI) mit diesem Prompt\n",
        "6. Erhalte generierte Antwort, die sich explizit auf unsere (internen) Daten stützt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM-Instantz (Chat)\n",
        "azure_openai = AzureChatOpenAI(\n",
        "    azure_deployment=azure_openai_completion_deployment_name\n",
        ")\n",
        "\n",
        "# Unser Prompt Template (in Deutsch, mit etwas Erklärung)\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"frage\", \"film_kontext\"],\n",
        "    template=\"\"\"\n",
        "    Du bist ein KI-System, das auf Basis der folgenden Filmdaten antworten soll.\n",
        "    Frage: {frage}\n",
        "    \n",
        "    Hier sind die relevanten Filminformationen, die als Kontext dienen (bitte nur daraus Fakten entnehmen und nutzen):\n",
        "    {film_kontext}\n",
        "    \n",
        "    Antworte bitte möglichst konkret.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Pipeline definieren\n",
        "chain = prompt_template | azure_openai | StrOutputParser()\n",
        "\n",
        "# Beispiel-Frage\n",
        "query = \"What are the best movies about superheroes? Please provide a synopsis.\"  \n",
        "\n",
        "# 1) Embeddings\n",
        "vect_query = azure_openai_embeddings.embed_query(query)\n",
        "vquery = VectorizedQuery(\n",
        "    vector=vect_query,\n",
        "    k_nearest_neighbors=5,\n",
        "    fields=\"vector\"\n",
        ")\n",
        "\n",
        "# 2) Suche in Azure AI Search (Semantisch + Vector)\n",
        "rag_results = search_client.search(\n",
        "    search_text=query,\n",
        "    vector_queries=[vquery],\n",
        "    query_type=\"semantic\",\n",
        "    semantic_configuration_name=\"movies-semantic-config\",\n",
        "    select=[\"original_title\", \"genre\", \"overview\"],\n",
        "    top=5\n",
        ")\n",
        "\n",
        "# Kontext für das Prompt extrahieren\n",
        "rag_list = list(rag_results)\n",
        "kompletter_kontext = \"\\n\".join([\n",
        "    f\"- Titel: {item['original_title']}, Genre: {item.get('genre','N/A')}, Info: {item.get('overview','Keine Beschreibung')}\"\n",
        "    for item in rag_list\n",
        "])\n",
        "\n",
        "# 3) Prompt ausfüllen & an LLM senden\n",
        "final_answer = chain.invoke({\"frage\": query, \"film_kontext\": kompletter_kontext})\n",
        "\n",
        "print(\"\\n=== GENERIERTE ANTWORT ===\\n\")\n",
        "print(final_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nun sollte das große Sprachmodell (Azure OpenAI) eine Antwort generieren, die sich auf unsere gefundene Filmliste bezieht (die wir aus dem Azure AI Search Index bekommen haben). Das ist das typische RAG-Prinzip.\n",
        "\n",
        "### Zusammenfassung\n",
        "1. Wir haben **bereits vorhandene Filmdaten** (z.B. in Cosmos DB) und daraus über das Azure Portal einen Azure AI Search Index erstellt.\n",
        "2. Wir nutzen Embeddings über Azure OpenAI, um eine Vektor-Suche durchzuführen.\n",
        "3. Im \"Hybrid\"-Ansatz ergänzen wir zusätzlich die Keyword-Suche + semantisches Reranking.\n",
        "4. Die Resultate fügen wir in das Prompt eines LLM ein, damit das LLM gezielt die Antwort formulieren kann, ohne \"Halluzinationen\".\n",
        "\n",
        "**Fertig!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ibit-hackathon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
